package ac.biu.cs.nlp.protec.algorithms.clusteringTerms;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.InputStreamReader;
import java.net.MalformedURLException;
import java.util.Collection;
import java.util.Hashtable;
import java.util.LinkedList;
import java.util.Set;



import ac.biu.cs.nlp.protec.data.DataOrganizer;
import ac.biu.cs.nlp.protec.data.lexicalExpansion.LexicalExpander;
import ac.biu.cs.nlp.protec.experiments.niceReasonUnification.NiceDataOrganizer;
import ac.biu.nlp.nlp.general.configuration.ConfigurationException;
import ac.biu.nlp.nlp.general.configuration.ConfigurationFile;
import ac.biu.nlp.nlp.general.configuration.ConfigurationFileDuplicateKeyException;
import ac.biu.nlp.nlp.general.configuration.ConfigurationParams;
import ac.biu.nlp.nlp.general.configuration.InitException;
import ac.biu.nlp.nlp.instruments.lemmatizer.LemmatizerException;
import ac.biu.nlp.nlp.representation.CanonicalPosTag;
import ac.biu.nlp.nlp.representation.PartOfSpeech;
import ac.biu.nlp.nlp.representation.UnspecifiedPartOfSpeech;
import edu.uci.ics.jung.algorithms.cluster.WeakComponentClusterer;
import edu.uci.ics.jung.graph.DirectedGraph;
import edu.uci.ics.jung.graph.DirectedSparseMultigraph;
import edu.uci.ics.jung.graph.util.Pair;


public class GraphBuilder {
	

/*	private static final String CORPUS_TOKENS_FILENAME = "D:\\NICE\\nov2011\\Stream_results\\streamTermsNoPOS.txt";
	private static final String INPUT_FILE = "D:\\NICE\\nov2011\\Stream_results\\eventsCorrect.txt";
	private static final String INPUT_FILE_GS = "D:/NICE/nov2011/Stream_results/GS_stream_2.txt";
*/

	private static final String CORPUS_TOKENS_FILENAME = "D:\\NICE\\nov2011\\citibank_results\\citibankTermsNoPOS.txt";
	private static final String INPUT_FILE = "D:\\NICE\\nov2011\\citibank_results\\eventsCorrect.txt";
	private static final String INPUT_FILE_GS = "D:/NICE/nov2011/citibank_results/GS_citibank_2.txt";

	
	private static final String LOG_FILE = INPUT_FILE.replace(".txt", "_entailments.log.txt");
	private static BufferedWriter logWriter=null; 
	private static final int MAX_RULES_NUMBER = 10;
	
	private static LexicalExpander LE = null;

	public DataOrganizer data = null; 
	private DirectedGraph<String, Integer> termEntailmentGraph = null;
	private DirectedGraph<Integer, Integer> sentenceEntailmentGraph = null;
	private LinkedList<String> initialTermsList=null;
	private LinkedList<String> expandedTermsList=null;
	
	private Hashtable<String,Integer> corpusDistionary = null;
	private boolean useExpansions;
	private boolean filterTermsNotFoundInReasons=false;
	
	public GraphBuilder(boolean useExpansions, String configurationFileName){
		this.useExpansions = useExpansions; 
		try {
			data = new NiceDataOrganizer(configurationFileName);
			data.loadData(new ConfigurationFile(configurationFileName));
		} catch (MalformedURLException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		} catch (ConfigurationException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		} catch (LemmatizerException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		System.out.println("Loaded initial data");
		initialTermsList = new LinkedList<String>();
		expandedTermsList = new LinkedList<String>();
	}	

	public GraphBuilder(boolean useExpansions, DataOrganizer dorg){
		this.useExpansions = useExpansions; 
		data = dorg;
		initialTermsList = new LinkedList<String>();
		expandedTermsList = new LinkedList<String>();
	}
	
	
	public DirectedGraph<String, Integer> getTermEntailmentGraph() {
		return termEntailmentGraph;
	}

	public DirectedGraph<Integer, Integer> getSentenceEntailmentGraph() {
		return sentenceEntailmentGraph;
	}

	
	public void loadCorpusDictionary(String filename) throws FileNotFoundException, IOException{
		BufferedReader reader = new BufferedReader(new FileReader(filename));
		String line = reader.readLine();	
		corpusDistionary = new Hashtable<String, Integer>();
		while(line != null) {
			String token = line.replace("\n","");
			if (!corpusDistionary.containsKey(token)){
				corpusDistionary.put(token,1);
			}
			line = reader.readLine();	
		}
		reader.close();
	}
	
	private boolean verifyEntailmentByCorpusDictionary(String lhs, String rhs){
		if (corpusDistionary.containsKey(lhs)&&(corpusDistionary.containsKey(rhs))) return  true;
		return false;
	}
	
	private LinkedList<String> getExpansions(String lhsTermLemma){
		if (useExpansions) return getExpandingTerms(lhsTermLemma);
		else return new LinkedList<String>();
	}
	
	private LinkedList<String> getExpandingTerms (String lhsTermLemma) {
		//System.out.println("Getting entailments for <<"+lhsTermLemma+">>");
		LinkedList<String> entailedTerms = new LinkedList<String>();

		try {
			PartOfSpeech n = new UnspecifiedPartOfSpeech(CanonicalPosTag.NOUN);
			PartOfSpeech v = new UnspecifiedPartOfSpeech(CanonicalPosTag.VERB);
			PartOfSpeech a = new UnspecifiedPartOfSpeech(CanonicalPosTag.ADJECTIVE);

			for (String expansion : LE.getExpansions(lhsTermLemma,n).keySet()){
				if (expansion.equals(lhsTermLemma)) continue;
				//if (!entailedTerms.contains(expansion)) entailedTerms.add(expansion);
				 entailedTerms.add(expansion);
			}
			for (String expansion : LE.getExpansions(lhsTermLemma,v).keySet()){
				if (expansion.equals(lhsTermLemma)) continue;
				//if (!entailedTerms.contains(expansion)) entailedTerms.add(expansion);
				 entailedTerms.add(expansion);
			}
			for (String expansion : LE.getExpansions(lhsTermLemma,a).keySet()){
				if (expansion.equals(lhsTermLemma)) continue;
				//if (!entailedTerms.contains(expansion)) entailedTerms.add(expansion);
				 entailedTerms.add(expansion);
			}
		} catch (Exception e1) {
			// TODO Auto-generated catch block
			e1.printStackTrace();
		}
		
		if (entailedTerms.size()<1) return entailedTerms;
		
		LinkedList<String> verifiedEntailedTerms=new LinkedList<String>();		
		try {
			for (String term:entailedTerms){
				if (verifyEntailmentByCorpusDictionary(lhsTermLemma, term)) {
					verifiedEntailedTerms.add(term);
					logWriter.write(lhsTermLemma+"\t"+term+"\t+\n");
				}
				else logWriter.write(lhsTermLemma+"\t"+term+"\t-\n");					
			}
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		return verifiedEntailedTerms;
	}
	

	public static DirectedGraph<String,Integer> cleanTermGraph(DirectedGraph<String,Integer> entailmentGraph, LinkedList<String> initialNodes){		
		
		Collection<String> terms = new LinkedList<String>();
		for(String t: entailmentGraph.getVertices()){
			terms.add(t);
		}
		
		for (String term : terms){
			if (initialNodes.contains(term)) continue;
			int numberOfInEdges=entailmentGraph.getInEdges(term).size();
			if (numberOfInEdges>1){
				System.out.println(term);
/*				for (String supportTerm:entailmentGraph.getPredecessors(term)){
					System.out.println("\t"+supportTerm);
				}
*/			}
			else {
				for(Integer edge:entailmentGraph.getInEdges(term)){
					entailmentGraph.removeEdge(edge);
				}
				entailmentGraph.removeVertex(term);
			}
		}	

		Collection<Integer> edges = new LinkedList<Integer>();
		for(Integer e: entailmentGraph.getEdges()){
			edges.add(e);
		}
		for (Integer edge : edges){
			Pair<String> endpoints=entailmentGraph.getEndpoints(edge);
			int num = entailmentGraph.findEdgeSet(endpoints.getFirst(), endpoints.getSecond()).size();
			num+=entailmentGraph.findEdgeSet(endpoints.getSecond(), endpoints.getFirst()).size();
			if (num>1) {
				System.out.println(">> "+endpoints);
			}
			else {
				for(Integer e:entailmentGraph.findEdgeSet(endpoints.getFirst(), endpoints.getSecond())){
					entailmentGraph.removeEdge(e);
				}
				for(Integer e:entailmentGraph.findEdgeSet(endpoints.getSecond(), endpoints.getFirst())){
					entailmentGraph.removeEdge(e);
				}
			}
		}	
		
		return entailmentGraph;
	}


	public static void printSentenceGraph(DirectedGraph<Integer,Integer> entailmentGraph,String outfilename) throws IOException{		
		System.out.println("------------------------/nPrinting sentence graph to "+outfilename);
		Hashtable<String,Integer> lines = new Hashtable<String, Integer>();
		BufferedWriter writer = new BufferedWriter(new FileWriter(outfilename));
		for (Integer edge : entailmentGraph.getEdges()){
			Pair<Integer> endpoints=entailmentGraph.getEndpoints(edge);
			//	System.out.println(">> "+endpoints);
			String line = endpoints.getFirst()+", "+endpoints.getSecond();
			if (!lines.containsKey(line)){
				lines.put(line,1);
			}
			else{
				lines.put(line, lines.get(line)+1);
			}
		}
		for (String line : lines.keySet()){
			writer.write(line+", "+lines.get(line)+"\n");
		}
		writer.close();		
	}	
	
	public static void printTermGraph(DirectedGraph<String,Integer> entailmentGraph,String outfilename) throws IOException{		
		System.out.println("------------------------/nPrinting term graph to "+outfilename);
		Hashtable<String,Integer> lines = new Hashtable<String, Integer>();
		BufferedWriter writer = new BufferedWriter(new FileWriter(outfilename));
		for (Integer edge : entailmentGraph.getEdges()){
			Pair<String> endpoints=entailmentGraph.getEndpoints(edge);
			//	System.out.println(">> "+endpoints);
			String line = endpoints.getFirst()+", "+endpoints.getSecond();
			if (!lines.containsKey(line)){
				lines.put(line,1);
			}
			else{
				lines.put(line, lines.get(line)+1);
			}
		}
		for (String line : lines.keySet()){
			writer.write(line+", "+lines.get(line)+"\n");
		}
		writer.close();	
		//DataGenerator.createGraphDataFile(entailmentGraph, outfilename+"_weka.arff");
	}	
	
	/**
	 * 
	 */
	public void buildEntailmentGraphs(String configurationFilename, boolean generateSentenceGraph) throws IOException {
		// TODO Auto-generated method stub
	
		DirectedGraph<String, Integer> initialEntailmentGraph=data.getInitialTermsGraph();

		loadCorpusDictionary(CORPUS_TOKENS_FILENAME);
		System.out.println("Loaded "+corpusDistionary.size()+" terms into corpusDict");
		termEntailmentGraph = new DirectedSparseMultigraph<String, Integer>();
		
			ConfigurationFile conf;
			
				try {
					conf = new ConfigurationFile(new File(configurationFilename));
					LE = new LexicalExpander(conf);
					System.out.println("Loaded LexicalExpander");

						logWriter = new BufferedWriter(new FileWriter(new File(LOG_FILE)));
					
					
					int edgeId=0;
					
					for(String term: initialEntailmentGraph.getVertices()){
						System.out.println(term);
						initialTermsList.add(term);
						expandedTermsList.add(term);
						Hashtable<Integer,Integer> reasonsWithCurrentTerm = data.getReasonsWithTerm(term);
						if(!termEntailmentGraph.containsVertex(term)) termEntailmentGraph.addVertex(term);
						for(String expandingdTerm : getExpansions(term)){
							if (filterTermsNotFoundInReasons) {
								if (!data.getReasonsByTerm().contains(expandingdTerm)){
									continue;
								}
							}
							// update expandedTermsList
							if(!expandedTermsList.contains(expandingdTerm)) expandedTermsList.add(expandingdTerm);
							// add nodes/edges to the term graph
							if(!termEntailmentGraph.containsVertex(expandingdTerm)) termEntailmentGraph.addVertex(expandingdTerm);
							termEntailmentGraph.addEdge(edgeId, term, expandingdTerm);
							System.out.println(edgeId+": "+term+"\t"+expandingdTerm);
							edgeId++;
							
							// extend initial reasons with entailed/entailing terms
							for (Integer rId : reasonsWithCurrentTerm.keySet()){
								data.updateExpandedDocsByTerm(expandingdTerm, rId,1);
							}
						} // end for (expandingTerm)
					}
					
					logWriter.close();
					

					
			//		termEntailmentGraph=cleanTermGraph(termEntailmentGraph,initialTermsList);
					printTermGraph(termEntailmentGraph,INPUT_FILE_GS.replace(".txt","_tGraph.txt"));
					
					
				if (generateSentenceGraph){
					// create sentence graph - reason ids as nodes
					sentenceEntailmentGraph = new DirectedSparseMultigraph<Integer, Integer>();
					for (Integer reasonId : data.getReasonsById().keySet()){
						sentenceEntailmentGraph.addVertex(reasonId);
					}
					edgeId=0;
					for(String term: initialTermsList){
						// connect each reason from the list with all the others from this list
						// i.e. connect reasons that share some term (before being expanded)
						LinkedList<Integer> reasonsWithCurrentTermList = new LinkedList<Integer>();
						Hashtable<Integer,Integer> reasonsWithCurrentTerm = data.getReasonsWithTerm(term);
						if (reasonsWithCurrentTerm!=null){
							for (Integer rId : reasonsWithCurrentTerm.keySet()){
								reasonsWithCurrentTermList.add(rId);
							}
						}
						
						for (int x=0; x<reasonsWithCurrentTermList.size(); x++){
							Integer xReasonId=reasonsWithCurrentTermList.get(x);
							// connect xReason with all the reasons following x in the list
							for (int y=x+1; y<reasonsWithCurrentTermList.size(); y++){
								Integer yReasonId=reasonsWithCurrentTermList.get(y);
								sentenceEntailmentGraph.addEdge(edgeId, xReasonId, yReasonId);
								edgeId++;
							}
						}
					}						
					printSentenceGraph(sentenceEntailmentGraph,INPUT_FILE_GS.replace(".txt","_sGraph_orig.txt"));
					
					for(String term: expandedTermsList){
						// connect each reason from the list with all the others from this list
						// after adding expansions, i.e. connect reasons that share some expanding term 
						if (initialTermsList.contains(term)) continue; // sentences with these were connected before
						else System.out.println("!!!! "+term);
						
						LinkedList<Integer> reasonsWithCurrentTermList = new LinkedList<Integer>();
						Hashtable<Integer,Integer> reasonsWithCurrentTerm = data.getReasonsWithTerm(term);
						if (reasonsWithCurrentTerm!=null){
							for (Integer rId : reasonsWithCurrentTerm.keySet()){
								reasonsWithCurrentTermList.add(rId);
							}
						}
						
						for (int x=0; x<reasonsWithCurrentTermList.size(); x++){
							Integer xReasonId=reasonsWithCurrentTermList.get(x);
							// connect xReason with all the reasons following x in the list
							for (int y=x+1; y<reasonsWithCurrentTermList.size(); y++){
								Integer yReasonId=reasonsWithCurrentTermList.get(y);
								sentenceEntailmentGraph.addEdge(edgeId, xReasonId, yReasonId);
								edgeId++;
							}
						}
					}					
					
					printSentenceGraph(sentenceEntailmentGraph,INPUT_FILE_GS.replace(".txt","_sGraph_expanded.txt"));
				}	
					WeakComponentClusterer<String, Integer> weakclust = new WeakComponentClusterer<String, Integer>();
					int id=1;
					System.out.println("-------------");
					for (Set<String> aset : weakclust.transform(termEntailmentGraph)){
						int size = aset.size();
						if (size<2) continue;
						System.out.println("Cluster "+id+" : size: "+size);					
						System.out.println(aset);
						System.out.println();	
						id++;
					}
					
					
					GraphToDot.printDotFie("D:/NICE/autoGraph.dot", termEntailmentGraph,initialTermsList);
					System.out.println(termEntailmentGraph.getVertexCount()+" Done graphToDot");
				} catch (ConfigurationFileDuplicateKeyException e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				} catch (ConfigurationException e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				} catch (InitException e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				} catch (IOException e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				} catch (Exception e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				}
				

	
	
	}


	
	/**
	 * @param args
	 */
	public static void main(String[] args) {
		boolean b_useExpansions=true;
		GraphBuilder gb = new GraphBuilder(b_useExpansions);
		try {
			gb.buildEntailmentGraphs(args[0],true);
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
	}

}
